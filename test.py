from typing import TypedDict, Annotated
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from PyPDF2 import PdfReader
import os

# å®šä¹‰ç±»å‹åŒ–çš„å·¥ä½œæµçŠ¶æ€
class WorkflowState(TypedDict):
    raw_text: str
    processed_chunks: Annotated[list[str], lambda x, y: x.extend(y)]
    extracted_data: dict

# åˆå§‹åŒ–æ¨¡å‹
llm = ChatOpenAI(
    model="gpt-4-turbo",
    temperature=0,
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    model_kwargs={"response_format": {"type": "json_object"}}
)

# è®¾è®¡ç»“æ„åŒ–æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "æ‚¨æ˜¯HSBCçš„å•†ä¸šæ•°æ®æå–ä¸“å®¶"),
    ("user", """è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–æ•°æ®ï¼š
      - company_name: å…¬å¸æ³•å®šåç§°
      - fiscal_year: è´¢æŠ¥å¹´ä»½
      - total_assets: æ€»èµ„äº§ï¼ˆè´§å¸å•ä½ï¼‰
      - risk_factors: åˆ—å‡ºTop3é£é™©å› ç´ 
      
      {text}
      
      ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡º""")
])

# åˆ›å»ºæ•°æ®å¤„ç†é“¾
analysis_chain = prompt | llm | JsonOutputParser()

def parse_pdf_node(state: WorkflowState):
    """PDFè§£æèŠ‚ç‚¹"""
    print("ğŸ“„ Extracting text from PDF...")
    reader = PdfReader("financial_report.pdf")
    return {"raw_text": "".join(page.extract_text() for page in reader.pages)}

def chunking_node(state: WorkflowState):
    """æ–‡æœ¬åˆ†å—å¤„ç†èŠ‚ç‚¹"""
    chunk_size = 3000
    text = state["raw_text"]
    return {"processed_chunks": [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]}

def assistant(state: WorkflowState):
    """AIæ•°æ®åˆ†æèŠ‚ç‚¹"""
    consolidated_data = {}
    
    for index, chunk in enumerate(state["processed_chunks"]):
        print(f"ğŸ” Analyzing chunk {index+1}/{len(state['processed_chunks'])}")
        
        try:
            chunk_result = analysis_chain.invoke({"text": chunk})
            
            # æ•°æ®åˆå¹¶ç­–ç•¥
            for key in ["company_name", "fiscal_year"]:
                if chunk_result.get(key) and not consolidated_data.get(key):
                    consolidated_data[key] = chunk_result[key]
                    
            if isinstance(chunk_result.get("total_assets"), (int, float)):
                consolidated_data["total_assets"] = max(
                    consolidated_data.get("total_assets", 0),
                    chunk_result["total_assets"]
                )
                
            consolidated_data.setdefault("risk_factors", []).extend(
                chunk_result.get("risk_factors", [])[:3]
                if isinstance(chunk_result.get("risk_factors"), list)
                else []
            )
            
        except Exception as e:
            print(f"âš ï¸ Error processing chunk {index+1}: {str(e)}")
    
    # å»é‡é£é™©å› ç´ 
    if "risk_factors" in consolidated_data:
        consolidated_data["risk_factors"] = list({v: None for v in consolidated_data["risk_factors"]}.keys())[:3]
    
    return {"extracted_data": consolidated_data}

# æ„å»ºå·¥ä½œæµ
workflow = StateGraph(WorkflowState)
workflow.add_node("parse_pdf", parse_pdf_node)
workflow.add_node("chunk_data", chunking_node)
workflow.add_node("ai_analyst", assistant)

workflow.set_entry_point("parse_pdf")
workflow.add_edge("parse_pdf", "chunk_data")
workflow.add_edge("chunk_data", "ai_analyst")
workflow.add_edge("ai_analyst", END)

app = workflow.compile()

# æµ‹è¯•æ‰§è¡Œ
if __name__ == "__main__":
    os.environ["OPENAI_API_KEY"] = "æ‚¨çš„å¯†é’¥"
    result = app.invoke({
        "raw_text": "",
        "processed_chunks": [],
        "extracted_data": {}
    })
    import json
    print(json.dumps(result["extracted_data"], indent=2, ensure_ascii=False))
